{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from mango import scheduler, Tuner\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    L1Loss,\n",
    "    BCEWithLogitsLoss,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch_geometric.nn import (\n",
    "    GINEConv,\n",
    "    GPSConv,\n",
    "    GraphNorm,\n",
    "    SAGPooling,\n",
    "    SetTransformerAggregation,\n",
    ")\n",
    "from rdkit import Chem\n",
    "from sklearn.feature_selection import r_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = catalog.load(\"merged_database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_list = []\n",
    "for mp in merged_df.MP.values:\n",
    "    if mp<=80:\n",
    "        mp=0\n",
    "    else:\n",
    "        mp=1\n",
    "    mp_list.append(mp)\n",
    "merged_df.MP = pd.Series(mp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from gnn_mp_model.pipelines.data_featurization.utils import from_smiles\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Descriptors import MolWt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "transform = T.AddRandomWalkPE(walk_length=30, attr_name=\"pe\")\n",
    "def _generate_graph_list(df: pd.DataFrame) -> t.List:\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        smiles = row[\"smiles\"]\n",
    "        label = row[\"MP\"]\n",
    "        graph = from_smiles(smiles)\n",
    "        graph.y = label\n",
    "        graph = transform(graph)\n",
    "        smiles_list = smiles.split(\".\")\n",
    "        if \"+\" in smiles_list[0]:\n",
    "            cation_smiles = smiles_list[0]\n",
    "            anion_smiles = smiles_list[1]\n",
    "        else:\n",
    "            cation_smiles = smiles_list[1]\n",
    "            anion_smiles = smiles_list[0]\n",
    "        cation = Chem.MolFromSmiles(cation_smiles)\n",
    "        anion = Chem.MolFromSmiles(anion_smiles)\n",
    "        cation_wt = MolWt(cation)\n",
    "        anion_wt = MolWt(anion)\n",
    "        cation_natoms = len(cation.GetAtoms())\n",
    "        cation_nbonds = len(cation.GetBonds())\n",
    "        anion_natoms = len(anion.GetAtoms())\n",
    "        anion_nbonds = len(anion.GetBonds())\n",
    "        graph.gf = torch.tensor(\n",
    "            [\n",
    "                cation_wt,\n",
    "                cation_natoms,\n",
    "                cation_nbonds,\n",
    "                anion_wt,\n",
    "                anion_natoms,\n",
    "                anion_nbonds,\n",
    "            ]\n",
    "        ).unsqueeze(0)\n",
    "        data_list.append(graph)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def generate_graph_loader(\n",
    "    df: pd.DataFrame, train: bool, batch_size: int\n",
    ") -> torch_geometric.loader.DataLoader:\n",
    "    data_list = _generate_graph_list(df)\n",
    "    graph_loader = DataLoader(\n",
    "        data_list,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return graph_loader\n",
    "\n",
    "\n",
    "def random_data_split(df: pd.DataFrame, split_ratio: float) -> t.Tuple:\n",
    "    df_train, df_test = train_test_split(df, test_size=split_ratio, random_state=42)\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = random_data_split(merged_df, split_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = generate_graph_loader(df_train, True, 32)\n",
    "test_dataloader = generate_graph_loader(df_train, False, 32)\n",
    "predict_dataloader = generate_graph_loader(merged_df, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, dense_size: int, num_layers:int, pooling: bool):  # noqa: PLR0913\n",
    "        # Loading params\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.pooling = pooling\n",
    "        node_dim = 9\n",
    "        edge_dim = 4\n",
    "        pe_dim = 8\n",
    "        gf_dim = 6\n",
    "        pool_rate = 0.65\n",
    "        # Initial embeddings\n",
    "        self.node_emb = Linear(pe_dim+node_dim, hidden_size)\n",
    "        self.pe_lin = Linear(30, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(30)\n",
    "        self.edge_emb = Linear(edge_dim, hidden_size)\n",
    "        self.aggr = SetTransformerAggregation(hidden_size)\n",
    "        # PNA\n",
    "        self.gps_list = ModuleList([])\n",
    "        self.gn_list = ModuleList([])\n",
    "        self.aggr_list = ModuleList([])\n",
    "        self.pool_list = ModuleList([])\n",
    "        # Initial layers\n",
    "        for _ in range(self.num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_size, hidden_size),\n",
    "                ReLU(),\n",
    "                Linear(hidden_size, hidden_size))\n",
    "            self.gps_list.append(GPSConv(hidden_size, GINEConv(nn, edge_dim=hidden_size), heads=4, dropout=0.2))\n",
    "            self.gn_list.append(GraphNorm(hidden_size))\n",
    "            self.aggr_list.append(SetTransformerAggregation(hidden_size))\n",
    "            self.pool_list.append(SAGPooling(hidden_size, pool_rate))\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = Linear(hidden_size+gf_dim, dense_size)\n",
    "        self.linear2 = Linear(dense_size, int(dense_size / 2))\n",
    "        self.linear3 = Linear(int(dense_size / 2), 1)\n",
    "\n",
    "    def forward(self, x, pe, edge_attr, edge_index, batch_index, gf):  # noqa: PLR0913\n",
    "        # Initial embeddings\n",
    "        x_pe = self.pe_norm(pe)\n",
    "        x = torch.cat((x, self.pe_lin(x_pe)), 1)\n",
    "        x = self.node_emb(x)\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "        global_representation = []\n",
    "        global_representation.append(self.aggr(x, batch_index))\n",
    "        ### Internal convolutions\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.gps_list[i](x, edge_index, batch_index, edge_attr=edge_attr)\n",
    "            x = self.gn_list[i](x, batch_index)\n",
    "            if self.pooling is True:\n",
    "                x, edge_index, edge_attr, batch_index, _, _ = self.pool_list[i](\n",
    "                x, edge_index, edge_attr, batch_index\n",
    "            )\n",
    "            global_representation.append(self.aggr_list[i](x, batch_index))\n",
    "        ### Output block\n",
    "        x = sum(global_representation)\n",
    "        x = torch.cat((x, gf), 1)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GNN(32,32,3,True)\n",
    "# x = model(batch.x.float(), batch.pe, batch.edge_attr.float(), batch.edge_index, batch.batch, batch.gf.float()).squeeze()\n",
    "# train_accuracy = Accuracy(task=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class GNN_L(L.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.model = GNN(params[\"hidden_size\"], params[\"dense_size\"], params[\"num_layers\"], params[\"pooling\"])\n",
    "        self.lr = params[\"lr\"]\n",
    "        self.weight_decay = params[\"weight_decay\"]\n",
    "        self.gamma = params[\"gamma\"]\n",
    "        self.loss_fn = BCEWithLogitsLoss()\n",
    "        self.train_accuracy = Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = Accuracy(task=\"binary\")\n",
    "        self.save_hyperparameters(params)\n",
    "\n",
    "    def forward(self, x, pe, edge_attr, edge_index, batch_index, gf):  # noqa: PLR0913\n",
    "        return self.model(x.float(), pe, edge_attr.float(), edge_index, batch_index, gf)\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        preds = self(batch.x.float(), batch.pe, batch.edge_attr.float(), batch.edge_index, batch.batch, batch.gf.float()).squeeze()\n",
    "        target = batch.y.float()\n",
    "        loss = self.loss_fn(preds, target)\n",
    "        acc = self.train_accuracy(preds, target)\n",
    "        self.log(\"CEL\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        preds = self(batch.x.float(), batch.pe, batch.edge_attr.float(), batch.edge_index, batch.batch, batch.gf.float()).squeeze()\n",
    "        target = batch.y.float()\n",
    "        acc = self.val_accuracy(preds, target)\n",
    "        self.log(\"val_acc\", acc)\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        preds = [self(batch.x.float(), batch.pe, batch.edge_attr.float(), batch.edge_index, batch.batch, batch.gf.float()).squeeze(), batch.smiles, batch.y]\n",
    "        return preds\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay = self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, gamma=self.gamma\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "\n",
    "def train_model(params):\n",
    "    filename = \"GPS_Main_Model_class\"\n",
    "    L.seed_everything(42)\n",
    "    model = GNN_L(params)\n",
    "    early_stopping = EarlyStopping(\"val_acc\", patience=10, mode=\"max\", strict=False)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "    checkpoint_callback = ModelCheckpoint(filename=\"{epoch}-{val_acc:.2f}\",\n",
    "                                          monitor=\"val_acc\",\n",
    "                                          save_top_k=2,\n",
    "                                          mode=\"max\")\n",
    "\n",
    "    logger = CSVLogger(save_dir=\"logs\", name=filename)\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=75,\n",
    "        callbacks=[early_stopping, lr_monitor, checkpoint_callback],\n",
    "        log_every_n_steps=20,\n",
    "        logger=logger,\n",
    "        deterministic=True,\n",
    "        accumulate_grad_batches=1\n",
    "    )\n",
    "    # Model pretraining\n",
    "    trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-05,\n",
    "    \"gamma\": 0.95,\n",
    "    \"hidden_size\": 32,\n",
    "    \"dense_size\": 64,\n",
    "    \"num_layers\": 5,\n",
    "    \"pooling\": False\n",
    "}\n",
    "model, trainer = train_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = GNN_L.load_from_checkpoint(\"all_data_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_df(model, predict_dataloader):\n",
    "    preds = trainer.predict(model, predict_dataloader)\n",
    "    preds_list = []\n",
    "    target_list = []\n",
    "    smiles_list = []\n",
    "    for item in preds:\n",
    "        preds_list.append(float(item[0]))\n",
    "        target_list.append(float(item[2]))\n",
    "        smiles_list.append(item[1][0])\n",
    "    df = pd.DataFrame(data=[preds_list, target_list, smiles_list]).transpose()\n",
    "    df.columns = [\"preds\", \"target\", \"smiles\"]\n",
    "    df[\"error\"] = abs(df[\"preds\"]- df[\"target\"]).astype(float)\n",
    "    return df\n",
    "\n",
    "def add_cation_anion_smiles(df):\n",
    "    cation_list = []\n",
    "    anion_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        smiles_list = row[\"smiles\"].split(\".\")\n",
    "        if \"+\" in smiles_list[0]:\n",
    "            cation_list.append(smiles_list[0])\n",
    "            anion_list.append(smiles_list[1])\n",
    "        else:\n",
    "            cation_list.append(smiles_list[1])\n",
    "            anion_list.append(smiles_list[0])\n",
    "\n",
    "    df[\"cation_smiles\"] = cation_list\n",
    "    df[\"anion_smiles\"] = anion_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predict_on_df(model_ckpt, predict_dataloader)\n",
    "df = add_cation_anion_smiles(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
